{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05297b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    CSVLoader,\n",
    "    PyPDFLoader,\n",
    "    DirectoryLoader,\n",
    "    WebBaseLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96370e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1\n",
      "\n",
      "Page Content Preview:\n",
      " The quick brown fox jumps over the lazy dog.\n",
      "This is the second line of the file.\n",
      "LangChain's TextLoader is great for simple text files.\n",
      "It handles various encodings like UTF-8.\n",
      "\n",
      "Metadata:\n",
      " {'source': 'data/sample.txt'}\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader(\"data/sample.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(\"Number of documents:\", len(documents))\n",
    "print(\"\\nPage Content Preview:\\n\", documents[0].page_content[:300])\n",
    "print(\"\\nMetadata:\\n\", documents[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "551533c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows converted to documents: 4\n",
      "\n",
      "Sample Document:\n",
      " Name: Alice\n",
      "Age: 30\n",
      "City: New York\n",
      "Occupation: Software Engineer\n"
     ]
    }
   ],
   "source": [
    "loader = CSVLoader(\"data/sample.csv\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(\"Number of rows converted to documents:\", len(documents))\n",
    "print(\"\\nSample Document:\\n\", documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fa2bc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages: 3\n",
      "\n",
      "Sample Page Content:\n",
      " 1. Cover Page \n",
      " Logo (from website) \n",
      " Title: Neuron – Corporate Photography & Video Solutions \n",
      " Tagline: Visual storytelling for modern brands \n",
      " Hero image from homepage \n",
      " \n",
      "2. About Neuron \n",
      " Introduction from homepage: \n",
      "“Elevate your brand's presence with our bespoke corporate video production and \n",
      "professional headshots in Pune. At Neuron, we help you unleash the power of visual \n",
      "storytelling to connect with your audience and showcase your corporate identity like never \n",
      "before.” \n",
      " Positio\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"data/sample.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(\"Total pages:\", len(documents))\n",
    "print(\"\\nSample Page Content:\\n\", documents[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "338cf3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents loaded from directory: 1\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(\n",
    "    \"data/\",\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader  # can customize per file type\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(\"Total documents loaded from directory:\", len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a20c515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Content Preview:\n",
      "\n",
      "LangChain overview - Docs by LangChainSkip to main contentDocs by LangChain home pageOpen sourceSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewDeep AgentsLangChainLangGraphIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewPrebuilt middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel\n"
     ]
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(\"Web Content Preview:\\n\")\n",
    "print(documents[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568bf1b4",
   "metadata": {},
   "source": [
    "##PART 2 —--- Text Splitters\n",
    "Task 6: Why Text Splitting is Required (Conceptual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaa5447",
   "metadata": {},
   "source": [
    "Why can’t we pass large documents directly?\n",
    "\n",
    "    LLMs have token limits\n",
    "\n",
    "    Large documents increase cost\n",
    "\n",
    "    Context window overflow\n",
    "\n",
    "    Lower accuracy in retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfed8ad",
   "metadata": {},
   "source": [
    "What does chunking solve?\n",
    "    Improves retrieval precision\n",
    "\n",
    "    Fits inside token limits\n",
    "\n",
    "    Reduces hallucination\n",
    "\n",
    "    Enables semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff0e7368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-text-splitters in .\\venv\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.13 in .\\venv\\lib\\site-packages (from langchain-text-splitters) (1.2.13)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in .\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in .\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (0.7.4)\n",
      "Requirement already satisfied: packaging>=23.2.0 in .\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in .\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (2.12.5)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in .\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in .\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (9.1.4)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in .\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in .\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in .\\venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in .\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in .\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (3.11.7)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in .\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in .\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (2.32.5)\n",
      "Requirement already satisfied: xxhash>=3.0.0 in .\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (3.6.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in .\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (0.25.0)\n",
      "Requirement already satisfied: anyio in .\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (4.12.1)\n",
      "Requirement already satisfied: certifi in .\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in .\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (1.0.9)\n",
      "Requirement already satisfied: idna in .\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in .\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in .\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in .\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in .\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in .\\venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (2.6.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in .\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-text-splitters) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ecfae47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2396, which is longer than the specified 500\n",
      "Created a chunk of size 1072, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 4\n",
      "\n",
      "Sample chunk:\n",
      " LangChain overview - Docs by LangChainSkip to main contentDocs by LangChain home pageOpen sourceSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewDeep AgentsLangChainLangGraphIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewPrebuilt middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this page Create an agent Core benefitsLangChain overviewCopy pageLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool — so you can build agents that adapt as fast as the ecosystem evolvesCopy pageLangChain is the easy way to start building completely custom agents and applications powered by LLMs.\n",
      "With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more.\n",
      "LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\n",
      "LangChain vs. LangGraph vs. Deep AgentsIf you are looking to build an agent, we recommend you start with Deep Agents which comes “batteries-included”, with modern features like automatic compression of long conversations, a virtual filesystem, and subagent-spawning for managing and isolating context.Deep Agents are implementations of LangChain agents. If you don’t need these capabilities or would like to customize your own for your agents and autonomous applications, start with LangChain.Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows and heavy customization.\n",
      "LangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n",
      "We recommend you use LangChain if you want to quickly build agents and autonomous applications.\n",
      "​ Create an agent\n",
      "Copy# pip install -qU langchain \"langchain[anthropic]\"\n",
      "from langchain.agents import create_agent\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(\"Number of chunks:\", len(chunks))\n",
    "print(\"\\nSample chunk:\\n\", chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0033e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 13\n",
      "\n",
      "Sample chunk:\n",
      " LangChain overview - Docs by LangChainSkip to main contentDocs by LangChain home pageOpen sourceSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewDeep AgentsLangChainLangGraphIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewPrebuilt middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(\"Number of chunks:\", len(chunks))\n",
    "print(\"\\nSample chunk:\\n\", chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6952075c",
   "metadata": {},
   "source": [
    "Splitter\t                    Advantage\t        Disadvantage\n",
    "CharacterTextSplitter\t        Simple\t            Breaks sentences\n",
    "RecursiveCharacterTextSplitter\tRespects structure\tSlightly slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7210cefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each page preserved as chunk\n",
      "Total pages: 3\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"data/sample.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "print(\"Each page preserved as chunk\")\n",
    "print(\"Total pages:\", len(pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43102195",
   "metadata": {},
   "source": [
    "Task 10: Semantic Meaning–Based Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf774a",
   "metadata": {},
   "source": [
    "What is Semantic Chunking?\n",
    "\n",
    "Instead of splitting by length, we split by meaning similarity.\n",
    "\n",
    "Chunks are created where:\n",
    "\n",
    "Topic shifts\n",
    "\n",
    "Semantic distance increases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd01c6d",
   "metadata": {},
   "source": [
    "How embeddings help?\n",
    "\n",
    "Convert text → vectors\n",
    "\n",
    "Measure similarity between sentences\n",
    "\n",
    "Break where similarity drops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e271035",
   "metadata": {},
   "source": [
    "PART 3 —--- Mini Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19929ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_and_split_documents(path_or_url):\n",
    "    \n",
    "    if path_or_url.startswith(\"http\"):\n",
    "        loader = WebBaseLoader(path_or_url)\n",
    "    else:\n",
    "        loader = DirectoryLoader(path_or_url, glob=\"**/*.txt\")\n",
    "    \n",
    "    documents = loader.load()\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_documents(documents)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbab9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "773c2c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks from local data: 1\n"
     ]
    }
   ],
   "source": [
    "chunks = load_and_split_documents(\"data/\")\n",
    "print(\"Total chunks from local data:\", len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a1f5962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks from web: 13\n"
     ]
    }
   ],
   "source": [
    "chunks = load_and_split_documents(\"https://python.langchain.com/docs/\")\n",
    "print(\"Total chunks from web:\", len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ed98b0",
   "metadata": {},
   "source": [
    "Which loader for which data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4798c5e5",
   "metadata": {},
   "source": [
    "Data Type\tLoader\n",
    ".txt\tTextLoader\n",
    ".csv\tCSVLoader\n",
    ".pdf\tPyPDFLoader\n",
    "Folder\tDirectoryLoader\n",
    "Website\tWebBaseLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7f47d3",
   "metadata": {},
   "source": [
    "Best Splitter For:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34363f28",
   "metadata": {},
   "source": [
    "Use Case\tBest Splitter\n",
    "Small text\tCharacterTextSplitter\n",
    "Large PDFs\tRecursiveCharacterTextSplitter\n",
    "Web data\tRecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ea4faa",
   "metadata": {},
   "source": [
    "Why chunk overlap is important?\n",
    "\n",
    "Maintains context continuity\n",
    "\n",
    "Prevents sentence breaking loss\n",
    "\n",
    "Improves retrieval quality\n",
    "\n",
    "Reduces answer fragmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
